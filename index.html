
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="img/logo_v2.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>MLPerf Inference Results Comparison</title>
      
    
    
  
      <link rel="stylesheet" href="assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/jquery.tablesorter.pager.min.css">
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/theme.blackice.min.css">
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/theme.blue.min.css">
<style type="text/css">

table.resultstable, table.counttable {
    overflow-x: auto;
}

.pager1{
    display: none!important;
}

.pagerSavedHeightSpacer {
    display: none!important;
}
.resultstable_wrapper, .counttable_wrapper {
    overflow-x: auto;
}

/* General form styling */
form {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #f5f5f5;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

/* Style for form group */
.form-group {
    margin-bottom: 20px;
    position: relative;
}

/* Label styling */
label {
    display: block;
    font-size: 14px;
    color: #333;
    margin-bottom: 5px;
    font-weight: 500;
}

/* Select styling */
select {
    width: 100%;
    padding: 10px;
    font-size: 16px;
    color: #333;
    background-color: #fff;
    border: 1px solid #ccc;
    border-radius: 4px;
    box-shadow: none;
    appearance: none;
    transition: border-color 0.3s ease;
}

select:focus {
    border-color: #6200ee;
    outline: none;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Custom arrow for select */
select::-ms-expand {
    display: none;
}

.select-wrapper {
    position: relative;
}

.select-wrapper::after {
    content: '';
    position: absolute;
    top: 50%;
    right: 10px;
    width: 0;
    height: 0;
    border-left: 5px solid transparent;
    border-right: 5px solid transparent;
    border-top: 5px solid #333;
    pointer-events: none;
    transform: translateY(-50%);
}

/* Button styling */
button {
    width: 100%;
    padding: 12px;
    font-size: 16px;
    color: #fff;
    background-color: #6200ee;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s ease;
}

button:hover {
    background-color: #3700b3;
}

button:active {
    background-color: #6200ee;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
}

button:disabled {
    background-color: #ccc;
    cursor: not-allowed;
}

/* Input and button ripple effect */
.button-ripple, .input-ripple {
    position: relative;
    overflow: hidden;
}

.button-ripple::after, .input-ripple::after {
    content: '';
    position: absolute;
    top: 50%;
    left: 50%;
    width: 100px;
    height: 100px;
    background: rgba(255, 255, 255, 0.4);
    border-radius: 50%;
    transform: translate(-50%, -50%) scale(0);
    transition: transform 0.5s ease, opacity 1s ease;
    opacity: 0;
    pointer-events: none;
}

.button-ripple:active::after, .input-ripple:focus::after {
    transform: translate(-50%, -50%) scale(1);
    opacity: 1;
    transition: 0s;
}

/* Responsive Design */
@media (max-width: 768px) {
    form {
        max-width: 100%;
        padding: 15px;
    }

    button {
        padding: 10px;
        font-size: 14px;
    }

    select {
        font-size: 14px;
        padding: 8px;
    }
}

@media (max-width: 480px) {
    form {
        padding: 10px;
    }

    button {
        padding: 8px;
        font-size: 14px;
    }

    select {
        font-size: 14px;
        padding: 8px;
    }

    label {
        font-size: 13px;
    }
}

select.pagesize, select.gotoPage {
    width: fit-content;
    padding: 5px;
}


/*Testing*/
/* Base Table Styles */
#results_table {
    margin: 20px 0;
    overflow-x: auto;
}

.tablesorter {
    width: 100%;
    border-collapse: collapse;
    background-color: #fff;
    font-size: 14px;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    overflow: hidden;
}

/* Header and Footer Styles */
.tablesorter thead th,
.tablesorter tfoot th {
    background-color: #6200ea; /* Material Design Purple */
    color: #ffffff;
    padding: 12px;
    text-align: left;
    text-transform: uppercase;
    font-weight: 500;
    border-bottom: 2px solid #512da8; /* Darker purple for bottom border */
}

.tablesorter tfoot th {
    background-color: #6200ea;
}

/* Body Row Styles */
.tablesorter tbody tr {
    border-bottom: 1px solid #e0e0e0;
    transition: background-color 0.3s ease;
}

.tablesorter tbody tr:nth-child(even) {
    background-color: #f9f9f9;
}

.tablesorter tbody tr:hover {
    background-color: #eeeeee;
}

.tablesorter tbody td {
    padding: 12px;
    text-align: left;
    color: #424242;
}

/* Specific Column Styles */
#col-id,
#col-system,
#col-submitter,
#col-accelerator {
    font-weight: bold;
}
#col-id, .col-id {
    min-width: 80px;
    max-width: 80px;
    width: 80px;
    left: 0px;
}
td.col-result {
    min-width: 100px;
    text-align: right!important;
}
td.col-system, th.col-system {
    min-width: 150px;
    max-width: 150px;
    width: 150px;
    word-break: break-word;
    left: 80px;
}
td.col-submitter, th.col-submitter {
    min-width: 120px;
    max-width: 120px;
    width: 120px;
    word-break: break-word;
    left: 230px;
}
td.col-accelerator, th.col-accelerator {
    min-width: 120px;
    max-width: 120px;
    width: 120px;
    word-break: break-word;
    left: 350px;
}

td.count-submitter, th.count-submitter {
    min-width: 150px;
    max-width: 150px;
    width: 150px;
    left: 0px;
    background: white;
    font-weight: 600;
    font-size: Large;
    word-break: break-word;
    position:sticky;
}
.col-scenario {
    font-weight: 500;
    color: #6200ea;
    text-align: center;
}

.col-result {
    text-align: right;
    color: #303f9f; /* Material Design Blue */
    font-weight: 600;
}

.headcol {
  position: sticky;
  background:white;
  border:none!important;
}

.collapsible {
    color: white;
    cursor: pointer;
    padding: 10px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
}

.chart {
    /* padding: 0 18px; */
    display: none; /* Hidden by default */
    overflow: hidden;
    height: 500px;
    width: 100%;
    background-color: white;
}


/* Responsive Design */
@media (max-width: 768px) {
    .tablesorter thead,
    .tablesorter tfoot {
        display: none;
    }

    .tablesorter tbody tr {
        display: block;
        margin-bottom: 15px;
        border: 1px solid #ddd;
        border-radius: 8px;
    }

    .tablesorter tbody td {
        display: flex;
        justify-content: space-between;
        padding: 10px;
        text-align: right;
    }

    .tablesorter tbody td::before {
        content: attr(data-label);
        font-weight: bold;
        color: #6200ea;
        text-transform: uppercase;
    }

    .tablesorter tbody td:first-child {
        border-top-left-radius: 8px;
        border-bottom-left-radius: 8px;
    }

    .tablesorter tbody td:last-child {
        border-top-right-radius: 8px;
        border-bottom-right-radius: 8px;
    }
}



</style>

  <!-- Add scripts that need to run afterwards here -->

    
  
      
    
<!-- load jQuery and tablesorter scripts -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>


<script type="text/javascript" src="thirdparty/tablesorter/dist/js/jquery.tablesorter.js"></script>
<!-- tablesorter widgets (optional) -->

<script type="text/javascript" src="thirdparty/tablesorter/dist/js/jquery.tablesorter.widgets.js"></script>
<script type="text/javascript" src="thirdparty/tablesorter/dist/js/extras/jquery.tablesorter.pager.min.js"></script>
  <!-- Add scripts that need to run before here -->
  <script src="https://cdn.canvasjs.com/ga/jquery.canvasjs.min.js"></script>
  
<script type="text/javascript" src="javascripts/config.js"></script>
<script type="text/javascript" src="javascripts/common.js"></script>
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="yellow">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="MLPerf Inference Results Comparison" class="md-header__button md-logo" aria-label="MLPerf Inference Results Comparison" data-md-component="logo">
      
  <img src="img/logo_v2.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MLPerf Inference Results Comparison
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Results
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/mlcommons/inference_results_v5.0" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="." class="md-tabs__link">
        
  
  
    
  
  Results

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="MLPerf Inference Results Comparison" class="md-nav__button md-logo" aria-label="MLPerf Inference Results Comparison" data-md-component="logo">
      
  <img src="img/logo_v2.svg" alt="logo">

    </a>
    MLPerf Inference Results Comparison
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mlcommons/inference_results_v5.0" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Results
    
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  



  
  


  <h1>Results</h1>

<html>

        <h2 id="results_heading_available" class="results_table_heading">Datacenter Category: Available submissions in Closed division</h2>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<div id="results_table_available" class="resultstable_wrapper"> <table class="resultstable tablesorter tableclosed tabledatacenter" id="results_available"><thead> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></thead><tfoot> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></tfoot>
        <tr>
        <td class="col-id headcol"> 5.0-0001 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/AMD/systems/8xMI325X_2xEPYC_9575F.json"> QuantaGrid D74A-7U </a> </td>
        <td class="col-submitter headcol"> AMD </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3E x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server"> 30724.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Offline"> 33928.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server"> 30724.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 33928.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0002 </td>
        <td class="col-system headcol" title="
Processor: 2xAMD EPYC 9655
Software: shark-ai 3.3.0
Cores per processor: 
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/AMD/systems/8xMI325x_2xEPYC-9655.json"> Quanta S7PA </a> </td>
        <td class="col-submitter headcol"> AMD </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3E x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8,fp16,int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/AMD/results/8xMI325x_2xEPYC-9655/stable-diffusion-xl/Server"> 16.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8,fp16,int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/AMD/results/8xMI325x_2xEPYC-9655/stable-diffusion-xl/Offline"> 17.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0003 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9475F 48-Core Processo
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/ASUSTeK/systems/ESC8000_H200_NVLx8_NVLink_TRT.json"> ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99/Server"> 27417.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99/Offline"> 27765.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99.9/Server"> 27417.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99.9/Offline"> 27765.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99/Server"> 17953.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99/Offline"> 18113.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99.9/Server"> 17953.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99.9/Offline"> 18113.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99/Server"> 525118.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99/Offline"> 326670.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99.9/Server"> 300029.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99.9/Offline"> 325823.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0004 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9475F 48-Core Processo
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/ASUSTeK/systems/ESC8000_H200_NVLx8_TRT.json"> ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/stable-diffusion-xl/Server"> 14.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/stable-diffusion-xl/Offline"> 16.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/retinanet/Server"> 11988.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/retinanet/Offline"> 11263.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/resnet50/Server"> 540125.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/resnet50/Offline"> 680738.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/3d-unet-99/Offline"> 49.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/3d-unet-99.9/Offline"> 49.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0005 </td>
        <td class="col-system headcol" title="
Processor: 2x AMD EPYC 9475F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/ASUSTeK/systems/ESC_A8A_MI325X_256GBx8.json"> ASUSTeK ESC A8A-E12U (8x MI325x-256GB ) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3E x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Server"> 30086.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Offline"> 33366.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Server"> 30086.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Offline"> 33366.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0006 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8558
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/ASUSTeK/systems/H200-SXM-141GBx8_TRT.json"> ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33037.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34807.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33037.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34807.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 21533.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21171.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21533.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21171.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0007 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: VMware ESXi, 8.0.3, 24501827 and vGPU_17.4_GA_AIE_ESXi_Host_Drivers for vGPUs
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Broadcom_Supermicro/systems/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT.json">  SYS-821GE-TNRT (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Broadcom_Supermicro </td>
        <td class="col-accelerator headcol"> Virtualized NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99/Server"> 19318.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 19936.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99.9/Server"> 19318.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 19936.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/retinanet/Server"> 12869.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/retinanet/Offline"> 14313.3 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 51.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 51.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0008 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86.0
Processors per node: 2
Nodes: 1
Notes: Cisco UCS C240 M8. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/1-node-2S-GNR_86C.json"> 1-node-2S-GNR_86C </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/gptj-99.9/Server"> 168.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/gptj-99.9/Offline"> 315.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/retinanet/Server"> 350.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/retinanet/Offline"> 476.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/resnet50/Server"> 27988.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/resnet50/Offline"> 31559.2 </a> </td>

                <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/1-node-2S-GNR_86C/3d-unet-99.9/Offline"> 2.3 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0009 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9224
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 24.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/C245M8_H100NVL_94GBx2_TRT.json"> Cisco UCS C245 M8 (2x H100NVL-PCIe-94GB) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 2 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/llama2-70b-99/Server"> 3337.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/llama2-70b-99/Offline"> 3741.3 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/gptj-99/Server"> 3305.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/gptj-99/Offline"> 3789.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/dlrm-v2-99/Server"> 96987.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/dlrm-v2-99/Offline"> 103047.0 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/retinanet/Server"> 2400.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/retinanet/Offline"> 2677.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/resnet50/Server"> 109989.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/resnet50/Offline"> 124595.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/3d-unet-99/Offline"> 10.8 </a> </td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0010 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9554 64-Core Processor
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 64.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/C885A_H100_SXMx8_TRT.json"> Cisco UCS C885A M8 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99/Server"> 26599.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99/Offline"> 30899.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99.9/Server"> 26599.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99.9/Offline"> 30899.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99/Server"> 20706.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99/Offline"> 20307.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99.9/Server"> 20706.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99.9/Offline"> 20307.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/stable-diffusion-xl/Server"> 17.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/stable-diffusion-xl/Offline"> 18.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/retinanet/Server"> 13908.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/retinanet/Offline"> 14476.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/resnet50/Server"> 652002.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_H100_SXMx8_TRT/resnet50/Offline"> 715761.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0011 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 128.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/C885A_M8_H200_SXM_141GBx8_TRT.json"> Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 30420.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 34081.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 30420.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 34081.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99/Server"> 21813.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99/Offline"> 21557.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99.9/Server"> 21813.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99.9/Offline"> 21557.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/resnet50/Server"> 648207.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/resnet50/Offline"> 770496.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0012 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9684X 128-Core Processor
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 64.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/L40Sx2_TRT.json"> UCS C245 M8 (2x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/L40Sx2_TRT/stable-diffusion-xl/Server"> 1.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/L40Sx2_TRT/stable-diffusion-xl/Offline"> 1.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/L40Sx2_TRT/dlrm-v2-99/Server"> 49591.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/L40Sx2_TRT/dlrm-v2-99/Offline"> 57832.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/L40Sx2_TRT/3d-unet-99/Offline"> 7.8 </a> </td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0013 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9224 24-Core Processor
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 64.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/X215M8-L40Sx2_TRT.json"> UCS X215 M8 (2x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8-L40Sx2_TRT/gptj-99/Server"> 1745.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8-L40Sx2_TRT/gptj-99/Offline"> 1755.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8-L40Sx2_TRT/dlrm-v2-99/Server"> 49591.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8-L40Sx2_TRT/dlrm-v2-99/Offline"> 57918.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8-L40Sx2_TRT/3d-unet-99/Offline"> 7.7 </a> </td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0014 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9554
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Cisco/systems/X215M8_H100NVLx2_TRT.json"> Cisco UCS X215c M8 with X440P PCIe node (2x H100NVL-PCIe-94GB) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 2 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/llama2-70b-99/Server"> 3337.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/llama2-70b-99/Offline"> 3879.9 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/gptj-99/Server"> 3701.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/gptj-99/Offline"> 3743.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/dlrm-v2-99/Server"> 98585.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/dlrm-v2-99/Offline"> 101768.0 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/retinanet/Server"> 2400.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/retinanet/Offline"> 2653.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/resnet50/Server"> 119990.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/resnet50/Offline"> 136731.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Cisco/results/X215M8_H100NVLx2_TRT/3d-unet-99/Offline"> 10.8 </a> </td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0015 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86.0
Processors per node: 2
Nodes: 1
Notes: Dell PowerEdge R670. INT4 for GPT-J, and INT8 for all other models
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/1-node-2S-GNR_86C.json"> 1-node-2S-GNR_86C </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/gptj-99.9/Server"> 168.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/gptj-99.9/Offline"> 316.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server"> 11788.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Offline"> 12397.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/retinanet/Server"> 350.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/retinanet/Offline"> 465.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/resnet50/Server"> 27987.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/resnet50/Offline"> 31472.7 </a> </td>

                <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/1-node-2S-GNR_86C/3d-unet-99.9/Offline"> 2.3 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0016 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9755
Software: TensorRT 10.8.0, CUDA 12.8
Cores per processor: 256.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/R7725_H100NVL_PCIE_94GBx2_TRT.json"> Dell PowerEdge R7725 (2x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/R7725_H100NVL_PCIE_94GBx2_TRT/3d-unet-99/Offline"> 10.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/R7725_H100NVL_PCIE_94GBx2_TRT/3d-unet-99.9/Offline"> 10.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0017 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9965 192-Core Processor
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 192.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/XE7745_H200_NVL_141GBx8_TRT.json"> Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Server"> 28848.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Offline"> 31149.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Server"> 28848.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Offline"> 31149.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99/Server"> 17975.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99/Offline"> 16900.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99.9/Server"> 17975.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99.9/Offline"> 16900.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0018 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9965 192-Core Processor
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 384.0
Processors per node: 2
Nodes: 1
Notes: L40S TGP 350W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/XE7745_L40Sx8_TRT.json"> Dell PowerEdge XE7745 (8x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99/Server"> 3201.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99/Offline"> 3481.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99.9/Server"> 3201.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99.9/Offline"> 3481.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99/Server"> 6213.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99/Offline"> 6946.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99.9/Server"> 6213.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99.9/Offline"> 6946.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/stable-diffusion-xl/Server"> 5.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/stable-diffusion-xl/Offline"> 6.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/retinanet/Server"> 6095.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/retinanet/Offline"> 6522.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/resnet50/Server"> 340047.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/resnet50/Offline"> 345721.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/3d-unet-99/Offline"> 28.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE7745_L40Sx8_TRT/3d-unet-99.9/Offline"> 28.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0019 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8580
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 120.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/XE9680L_H200_SXM_141GBx8_TRT.json"> Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 33023.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 34763.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 33023.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 34763.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99/Server"> 21455.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99/Offline"> 21406.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99.9/Server"> 21455.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99.9/Offline"> 21406.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/stable-diffusion-xl/Server"> 16.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/stable-diffusion-xl/Offline"> 19.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/retinanet/Server"> 13588.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/retinanet/Offline"> 14794.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/resnet50/Server"> 632200.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/resnet50/Offline"> 765417.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/3d-unet-99/Offline"> 54.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/3d-unet-99.9/Offline"> 54.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0020 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/XE9680_H100_SXM_80GBx8_TRT.json"> Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99/Server"> 30903.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99/Offline"> 31216.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server"> 30903.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 31216.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99/Server"> 21134.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 20663.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99.9/Server"> 21134.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 20663.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 51.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 51.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0021 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/XE9680_H200_SXM_141GBx8_TRT.json"> Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 33051.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 34822.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 33051.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 34822.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99/Server"> 21536.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99/Offline"> 21117.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99.9/Server"> 21536.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99.9/Offline"> 21117.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/stable-diffusion-xl/Server"> 16.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/stable-diffusion-xl/Offline"> 19.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/retinanet/Server"> 13588.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/retinanet/Offline"> 14893.4 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/3d-unet-99/Offline"> 54.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/3d-unet-99.9/Offline"> 54.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0022 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon Platinum 8468
Software: TensorRT 10.8, CUDA 12.4
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: VM Specifications 64vCPU out of 96 and memory of 1TB GB out of 2TB
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Dell/systems/vXE9680_H100_SXM_80GBx8_TRT.json"> Dell PowerEdge XE9680 (8x H100-SXM-80GB,VMware ESXi 8.0.3)  </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> Virtualized NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0023 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) GOLD 6530
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 32.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Fujitsu/systems/H100-NVL-94GBx8_TRT.json"> PRIMERGY CDI (8x H100NVL, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Fujitsu </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/llama2-70b-99/Server"> 16848.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/llama2-70b-99/Offline"> 18084.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/llama2-70b-99.9/Server"> 16848.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/llama2-70b-99.9/Offline"> 18084.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/gptj-99/Server"> 14846.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/gptj-99/Offline"> 14769.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/gptj-99.9/Server"> 14846.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/gptj-99.9/Offline"> 14769.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/retinanet/Server"> 10234.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/retinanet/Offline"> 11106.2 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/3d-unet-99/Offline"> 44.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Fujitsu/results/H100-NVL-94GBx8_TRT/3d-unet-99.9/Offline"> 44.6 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0024 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) w7-2495X
Software: TensorRT
Cores per processor: 24.0
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/GATEOverflow/systems/RTX4090x2-nvidia-gpu-TensorRT-default_config.json"> GATE Overflow Intel Sapphire Rapids (2x RTX 4090) </a> </td>
        <td class="col-submitter headcol"> GATEOverflow </td>
        <td class="col-accelerator headcol"> NVIDIA GeForce RTX 4090 x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>
                    <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GATEOverflow/results/RTX4090x2-nvidia-gpu-TensorRT-default_config/bert-99/offline"> 8266.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GATEOverflow/results/RTX4090x2-nvidia-gpu-TensorRT-default_config/bert-99.9/offline"> 3339.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GATEOverflow/results/RTX4090x2-nvidia-gpu-TensorRT-default_config/resnet50/server"> 73725.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GATEOverflow/results/RTX4090x2-nvidia-gpu-TensorRT-default_config/resnet50/offline"> 87921.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GATEOverflow/results/RTX4090x2-nvidia-gpu-TensorRT-default_config/3d-unet-99/offline"> 8.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GATEOverflow/results/RTX4090x2-nvidia-gpu-TensorRT-default_config/3d-unet-99.9/offline"> 8.3 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0033 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/GigaComputing/systems/G893-SD1_H200-SXM-141GBx8_TRT.json"> G893-SD1 </a> </td>
        <td class="col-submitter headcol"> GigaComputing </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33054.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34889.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33054.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34889.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99/Server"> 21544.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21450.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21544.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21450.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/stable-diffusion-xl/Server"> 16.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 19.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 585166.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 645975.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server"> 370056.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 388769.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/retinanet/Server"> 14004.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/retinanet/Offline"> 14992.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/resnet50/Server"> 676219.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/resnet50/Offline"> 765403.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/3d-unet-99/Offline"> 54.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/3d-unet-99.9/Offline"> 54.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0034 </td>
        <td class="col-system headcol" title="
Processor: 2xAMD EPY 9175F
Software: ROCm 6.3.3
Cores per processor: 16.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/GigaComputing/systems/G893-ZX1-AAX2.json"> G893-ZX1-AAX2 </a> </td>
        <td class="col-submitter headcol"> GigaComputing </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3E x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99/Server"> 29094.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99/Offline"> 33251.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99.9/Server"> 29094.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99.9/Offline"> 33251.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0035 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8581C CPU
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W. TensorRT LLM
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Google/systems/H200-SXM-141GBx8_TRT.json"> a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 31390.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34144.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 31390.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34144.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 20501.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21040.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 20501.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21040.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 555883.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 634294.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server"> 362655.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 383039.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/retinanet/Server"> 12909.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/retinanet/Offline"> 14765.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0036 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9B14
Software: flax
Cores per processor: 180.0
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Google/systems/trillium_x4_flax.json"> tpu-v6e-4 </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> TPU Trillium x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: bf16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/trillium_x4_flax/stable-diffusion-xl/Server"> 5.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: bf16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/trillium_x4_flax/stable-diffusion-xl/Offline"> 6.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0037 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.8, CUDA 12.7
Cores per processor: 72.0
Processors per node: 1
Nodes: 1
Notes: NVIDIA GH200 144GB HBM3e
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT.json"> HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA GH200 Grace Hopper Superchip 144GB x 1 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Server"> 4327.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Offline"> 4609.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Server"> 4327.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Offline"> 4609.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Server"> 2526.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Offline"> 2845.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Server"> 2526.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Offline"> 2845.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/stable-diffusion-xl/Server"> 2.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 2.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Server"> 80994.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Offline"> 85998.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Server"> 50070.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 51970.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0038 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 144.0
Processors per node: 2
Nodes: 1
Notes: NVIDIA GH200 144GB HBM3e
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT.json"> HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA GH200 Grace Hopper Superchip 144GB x 2 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99/Server"> 8674.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99/Offline"> 9362.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99.9/Server"> 8674.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99.9/Offline"> 9362.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99/Server"> 5039.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99/Offline"> 5666.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99.9/Server"> 5039.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99.9/Offline"> 5666.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/stable-diffusion-xl/Server"> 4.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/stable-diffusion-xl/Offline"> 5.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99/Server"> 157961.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99/Offline"> 173943.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99.9/Server"> 98162.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99.9/Offline"> 103949.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0039 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT.json"> HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99/Server"> 30457.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99/Offline"> 31238.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99.9/Server"> 30500.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99.9/Offline"> 31252.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99/Server"> 19363.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99/Offline"> 20993.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99.9/Server"> 19363.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99.9/Offline"> 20796.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/stable-diffusion-xl/Server"> 16.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/stable-diffusion-xl/Offline"> 18.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99/Server"> 555631.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99/Offline"> 599191.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99.9/Server"> 358055.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99.9/Offline"> 377305.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/retinanet/Server"> 13748.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/retinanet/Offline"> 14637.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/resnet50/Server"> 620188.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/resnet50/Offline"> 718177.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/3d-unet-99/Offline"> 52.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/3d-unet-99.9/Offline"> 52.1 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0040 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_Cray_XD670_H100_SXM_80GBx8_TRT.json"> HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99/Server"> 30498.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99/Offline"> 31174.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server"> 30480.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 31147.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99/Server"> 20991.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 20925.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99.9/Server"> 20795.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 20716.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Server"> 16.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 18.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/retinanet/Server"> 13748.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/retinanet/Offline"> 14633.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/resnet50/Server"> 620188.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/resnet50/Offline"> 717848.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 52.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 52.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0041 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8562Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 32.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_Cray_XD670_H200_SXM_141GBx8_TRT.json"> HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 31281.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 33781.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 31283.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 33964.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99/Server"> 21025.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99/Offline"> 21324.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99.9/Server"> 21019.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99.9/Offline"> 21014.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/stable-diffusion-xl/Server"> 16.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/stable-diffusion-xl/Offline"> 19.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99/Server"> 590167.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99/Offline"> 654489.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99.9/Server"> 380059.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99.9/Offline"> 398164.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/retinanet/Server"> 14388.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/retinanet/Offline"> 14909.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/resnet50/Server"> 667216.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/resnet50/Offline"> 769420.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/3d-unet-99/Offline"> 55.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/3d-unet-99.9/Offline"> 55.1 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0042 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8468
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 192.0
Processors per node: 4
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_Hawks_3200_H100_NVL_94GBx4_TRT.json"> HPE Compute Scale-up Server 3200 (4x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/gptj-99/Server"> 7179.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/gptj-99/Offline"> 7240.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/retinanet/Server"> 5097.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/retinanet/Offline"> 5326.9 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/3d-unet-99/Offline"> 21.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/3d-unet-99.9/Offline"> 21.9 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0043 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6740E
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 192.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT.json"> HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99/Server"> 27638.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99/Offline"> 30297.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99.9/Server"> 27638.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99.9/Offline"> 30297.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99/Server"> 18065.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99/Offline"> 17882.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99.9/Server"> 18065.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99.9/Offline"> 17882.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/stable-diffusion-xl/Server"> 15.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/stable-diffusion-xl/Offline"> 16.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99/Server"> 201011.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99/Offline"> 512363.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99.9/Server"> 195518.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99.9/Offline"> 320287.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/retinanet/Server"> 11988.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/retinanet/Offline"> 13549.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/resnet50/Server"> 620188.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/resnet50/Offline"> 686884.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/3d-unet-99/Offline"> 49.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/3d-unet-99.9/Offline"> 49.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0044 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6740E
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 96.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT.json"> HPE ProLiant Compute DL380a Gen12 (8x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/dlrm-v2-99.9/Server"> 170018.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/dlrm-v2-99.9/Offline"> 225640.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/retinanet/Server"> 9991.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/retinanet/Offline"> 10893.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/resnet50/Server"> 480097.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/resnet50/Offline"> 483928.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/3d-unet-99/Offline"> 43.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/3d-unet-99.9/Offline"> 43.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0045 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6740E
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 96.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT.json"> HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99/Server"> 6821.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99/Offline"> 6823.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99.9/Server"> 6821.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99.9/Offline"> 6823.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/stable-diffusion-xl/Server"> 5.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/stable-diffusion-xl/Offline"> 6.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99/Server"> 177643.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99/Offline"> 225297.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99.9/Server"> 94989.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99.9/Offline"> 100517.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/retinanet/Server"> 6095.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/retinanet/Offline"> 6305.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/resnet50/Server"> 344052.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/resnet50/Offline"> 330082.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/3d-unet-99/Offline"> 27.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/3d-unet-99.9/Offline"> 27.1 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0046 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6740E
Software: TensorRT 9.0, CUDA 12.8
Cores per processor: 96.0
Processors per node: 2
Nodes: 1
Notes: Llama2-70b used MLPerf v4.1 TensorRT-LLM instead
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/HPE/systems/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0.json"> HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99/Server"> 3181.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99/Offline"> 3655.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99.9/Server"> 3181.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99.9/Offline"> 3655.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0048 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6980P
Software: PyTorch
Cores per processor: 128.0
Processors per node: 2
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Intel/systems/1-node-2S-GNR_128C.json"> 1-node-2S-GNR_128C </a> </td>
        <td class="col-submitter headcol"> Intel </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/gptj-99/Server"> 265.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/gptj-99/Offline"> 516.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/gptj-99.9/Server"> 265.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/gptj-99.9/Offline"> 516.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99/Server"> 18177.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99/Offline"> 18686.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server"> 18177.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Offline"> 18686.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/retinanet/Server"> 580.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/retinanet/Offline"> 713.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/resnet50/Server"> 40184.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/resnet50/Offline"> 45530.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/3d-unet-99/Offline"> 3.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/3d-unet-99.9/Offline"> 3.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0049 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Lambda/systems/H200-SXM-141GBx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lambda </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33045.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 35385.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33045.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 35385.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 21543.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21448.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21543.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21448.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 585164.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 643756.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server"> 370055.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 386569.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0050 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6787P
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 86.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Lenovo/systems/H100-NVL-94GBx4_TRT.json"> ThinkSystem SR650a V4 (4x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99/Server"> 6773.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99/Offline"> 7706.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99.9/Server"> 6773.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99.9/Offline"> 7706.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/stable-diffusion-xl/Server"> 5.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/stable-diffusion-xl/Offline"> 6.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/retinanet/Server"> 5277.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/retinanet/Offline"> 5487.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/resnet50/Server"> 265225.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/resnet50/Offline"> 278849.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/3d-unet-99/Offline"> 22.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H100-NVL-94GBx4_TRT/3d-unet-99.9/Offline"> 22.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0051 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Lenovo/systems/H200-SXM-141GBx8_TRT.json"> ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 32976.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 35453.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 32976.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 35453.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 21549.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21513.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21549.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21513.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/stable-diffusion-xl/Server"> 16.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 19.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/retinanet/Server"> 13588.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/retinanet/Offline"> 14785.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/resnet50/Server"> 673220.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/resnet50/Offline"> 773300.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/3d-unet-99/Offline"> 54.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/H200-SXM-141GBx8_TRT/3d-unet-99.9/Offline"> 54.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0052 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9655 96-Core Processor
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 96.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Lenovo/systems/SR675v3_H200_SXMx4_TRT.json"> ThinkSystem SR675 V3 (4x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99/Server"> 14652.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99/Offline"> 17654.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99.9/Server"> 14652.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99.9/Offline"> 17654.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99/Server"> 10503.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99/Offline"> 10825.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99.9/Server"> 10503.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99.9/Offline"> 10825.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/stable-diffusion-xl/Server"> 8.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/stable-diffusion-xl/Offline"> 9.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/retinanet/Server"> 6795.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/retinanet/Offline"> 7239.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/resnet50/Server"> 304031.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/resnet50/Offline"> 378623.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/3d-unet-99/Offline"> 27.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/3d-unet-99.9/Offline"> 27.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0053 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Lenovo/systems/SR780a_V3_H200SXMx8.json"> ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99/Server"> 32934.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99/Offline"> 35400.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99.9/Server"> 32934.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99.9/Offline"> 35400.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99/Server"> 21553.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99/Offline"> 21626.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99.9/Server"> 21553.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99.9/Offline"> 21626.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/stable-diffusion-xl/Server"> 16.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/stable-diffusion-xl/Offline"> 19.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/retinanet/Server"> 13996.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/retinanet/Offline"> 14711.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/resnet50/Server"> 670220.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/resnet50/Offline"> 771137.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/3d-unet-99/Offline"> 54.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Lenovo/results/SR780a_V3_H200SXMx8/3d-unet-99.9/Offline"> 54.6 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0054 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9534
Software: ROCm 6.3.0
Cores per processor: 128.0
Processors per node: 2
Nodes: 4
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/MangoBoost/systems/32xMI300X_2xEPYC_9534.json"> MangoBoost Mi300X (32x MI300X-192GB-HBM3, LLMBoost) </a> </td>
        <td class="col-submitter headcol"> MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99/Server"> 93039.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99/Offline"> 103182.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99.9/Server"> 93039.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99.9/Offline"> 103182.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0055 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/B200-SXM-180GBx1_TRT.json"> NVIDIA DGX B200 (1x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 1 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99/Server"> 12078.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99/Offline"> 12269.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99.9/Server"> 12078.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99.9/Offline"> 12269.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0056 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/B200-SXM-180GBx8_TRT.json"> NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 98443.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 98858.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 98443.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 98858.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 30.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0057 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H100 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/DGX-H100_H100-SXM-80GBx8_TRT.json"> NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99/Server"> 31106.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99/Offline"> 31306.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Server"> 31106.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 31306.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0058 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 144.0
Processors per node: 2
Nodes: 18
Notes: NVIDIA GB200 NVL72
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/GB200-NVL72_GB200-186GB_aarch64x72_TRT_Triton.json">  (72x GB200-186GB_aarch64, TensorRT, Triton) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0059 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 144.0
Processors per node: 1
Nodes: 1
Notes: NVIDIA GH200 144GB HBM3e
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT.json"> HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GH200 Grace Hopper Superchip 144GB x 1 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Server"> 4350.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Offline"> 4686.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Server"> 4350.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Offline"> 4686.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Server"> 2526.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Offline"> 2824.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Server"> 2526.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Offline"> 2824.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/stable-diffusion-xl/Server"> 2.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 2.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Server"> 80994.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Offline"> 87268.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Server"> 50991.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 52549.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0060 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/H200-SXM-141GBx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33071.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34988.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33071.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34988.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 21544.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21182.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21544.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21182.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/stable-diffusion-xl/Server"> 18.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 19.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 585164.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 641629.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server"> 370054.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 385327.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/retinanet/Server"> 13588.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/retinanet/Offline"> 14568.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/resnet50/Server"> 632194.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/resnet50/Offline"> 760715.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/3d-unet-99/Offline"> 54.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/3d-unet-99.9/Offline"> 54.6 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0061 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/NVIDIA/systems/H200-SXM-141GBx8_TRT_Triton.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT, Triton) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99/Server"> 33024.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99/Offline"> 34300.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99.9/Server"> 33024.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99.9/Offline"> 34300.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0062 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Oracle/systems/H200-SXM-141GBx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Oracle </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33001.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34556.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33001.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34556.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 21527.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 21104.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21527.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 21104.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/retinanet/Server"> 13588.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/retinanet/Offline"> 14693.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/resnet50/Server"> 632198.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Oracle/results/H200-SXM-141GBx8_TRT/resnet50/Offline"> 755069.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0063 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86.0
Processors per node: 2
Nodes: 1
Notes: QuantaGrid-D55X-1U. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Quanta_Cloud_Technology/systems/1-node-2S-GNR_86C.json"> 1-node-2S-GNR_86C </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/gptj-99.9/Server"> 168.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/gptj-99.9/Offline"> 300.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server"> 11789.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Offline"> 12391.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/retinanet/Server"> 350.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/retinanet/Offline"> 456.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/resnet50/Server"> 27987.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/resnet50/Offline"> 31091.4 </a> </td>

                <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/3d-unet-99.9/Offline"> 2.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0064 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8470
Software: TensorRT 10.8.0.43, CUDA 12.8
Cores per processor: 52.0
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D54U-3U
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Quanta_Cloud_Technology/systems/D54U_3U_H100_PCIe_80GBx4_TRT.json"> QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA H100-PCIe-80GB x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99/Server"> 4889.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99/Offline"> 7013.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99.9/Server"> 4889.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99.9/Offline"> 7013.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99/Server"> 6251.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99/Offline"> 6319.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99.9/Server"> 6251.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99.9/Offline"> 6319.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/stable-diffusion-xl/Server"> 4.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/stable-diffusion-xl/Offline"> 5.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99/Server"> 175419.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99/Offline"> 187064.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99.9/Server"> 100187.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99.9/Offline"> 109408.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/retinanet/Server"> 4440.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/retinanet/Offline"> 4709.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/resnet50/Server"> 188014.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/resnet50/Offline"> 226268.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/3d-unet-99/Offline"> 18.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/3d-unet-99.9/Offline"> 18.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0065 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Quanta_Cloud_Technology/systems/D74U_7U_H100_SXM_80GBx8_TRT.json"> QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99/Server"> 29916.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99/Offline"> 31083.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server"> 29916.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 31083.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99/Server"> 19316.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 20464.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99.9/Server"> 19316.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 20464.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Server"> 16.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 18.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99/Server"> 512121.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99/Offline"> 586194.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Server"> 340044.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Offline"> 372632.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/retinanet/Server"> 12869.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/retinanet/Offline"> 14356.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/resnet50/Server"> 584161.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/resnet50/Offline"> 712859.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 51.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 51.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0066 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 72.0
Processors per node: 1
Nodes: 1
Notes: QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Quanta_Cloud_Technology/systems/S74G_2U_GH200_96GB_aarch64x1_TRT.json"> QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA GH200 Grace Hopper Superchip 96GB x 1 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99/Server"> 3203.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99/Offline"> 3144.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99.9/Server"> 3203.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99.9/Offline"> 3144.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99/Server"> 2824.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99/Offline"> 2808.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99.9/Server"> 2824.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99.9/Offline"> 2808.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/stable-diffusion-xl/Server"> 1.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 2.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99/Server"> 82792.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99/Offline"> 83504.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99.9/Server"> 45990.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 50606.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/retinanet/Server"> 1730.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/retinanet/Offline"> 1925.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/resnet50/Server"> 72995.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/resnet50/Offline"> 96157.6 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0067 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6980P
Software: PyTorch
Cores per processor: 128.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/1-node-2S-GNR_128C.json"> SYS-822GA-NGR3 </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/gptj-99.9/Server"> 251.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/gptj-99.9/Offline"> 493.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server"> 18987.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Offline"> 19577.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/retinanet/Server"> 585.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/retinanet/Offline"> 726.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/resnet50/Server"> 40285.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/resnet50/Offline"> 46041.4 </a> </td>

                <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/3d-unet-99.9/Offline"> 3.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0068 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.0.0.dev1+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/8xMI325X_2xEPYC_9575F.json"> AS-8126GS-TNMR </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3E x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server"> 28770.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Offline"> 33069.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server"> 28770.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: Dummy" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 33069.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0069 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: CUDA 12.8
Cores per processor: 72.0
Processors per node: 1
Nodes: 1
Notes: NVIDIA GH200 96GB HBM3
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/ARS_111GL_NHR_TRT.json"> ARS-111GL-NHR (1x GH200-96GB_aarch64) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA GH200 Grace Hopper Superchip 96GB x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99/Server"> 80992.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99/Offline"> 85986.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99.9/Server"> 50468.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99.9/Offline"> 51030.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0070 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9654
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 96.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT.json"> AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99/Server"> 33045.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99/Offline"> 35026.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Server"> 33045.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Offline"> 35026.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99/Server"> 21757.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99/Offline"> 21477.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99.9/Server"> 21757.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99.9/Offline"> 21477.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/stable-diffusion-xl/Server"> 18.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/stable-diffusion-xl/Offline"> 19.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/retinanet/Server"> 14589.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/retinanet/Offline"> 15200.3 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/3d-unet-99/Offline"> 54.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/3d-unet-99.9/Offline"> 54.6 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0071 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6979P
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 120.0
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/SYS522GA_H200X8_NVL_TRT.json"> SYS-522GA-NRT(8x H200-NVL-141GB) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99/Server"> 18571.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99/Offline"> 18724.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99.9/Server"> 18571.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99.9/Offline"> 18724.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/stable-diffusion-xl/Server"> 15.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/stable-diffusion-xl/Offline"> 16.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/retinanet/Server"> 11829.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/retinanet/Offline"> 13802.9 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/3d-unet-99/Offline"> 49.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/3d-unet-99.9/Offline"> 49.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0072 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8562Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 32.0
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT.json"> SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99/Server"> 97629.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99/Offline"> 98782.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server"> 97629.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Offline"> 98782.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0073 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT.json"> SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99/Server"> 33052.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99/Offline"> 34628.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Server"> 33052.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Offline"> 34628.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99/Server"> 21544.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99/Offline"> 21304.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99.9/Server"> 21544.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99.9/Offline"> 21304.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/stable-diffusion-xl/Server"> 18.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/stable-diffusion-xl/Offline"> 19.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/retinanet/Server"> 14488.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/retinanet/Offline"> 14955.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/resnet50/Server"> 640203.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/resnet50/Offline"> 759826.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/3d-unet-99/Offline"> 54.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/3d-unet-99.9/Offline"> 54.6 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0074 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48.0
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Supermicro/systems/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT.json"> SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99/Server"> 97552.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99/Offline"> 97416.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server"> 97552.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Offline"> 97416.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/stable-diffusion-xl/Server"> 28.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/stable-diffusion-xl/Offline"> 30.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0075 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8462Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 32.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Sustainable_Metal_Cloud/systems/H200-SXM-141GBx8_TRT.json"> Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Sustainable_Metal_Cloud </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33027.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34264.0 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99/Server"> 21359.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 20999.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server"> 21359.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 20999.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 570145.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 631064.0 </a> </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/retinanet/Server"> 13588.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/retinanet/Offline"> 14391.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/resnet50/Server"> 632197.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/resnet50/Offline"> 749810.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/3d-unet-99/Offline"> 54.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/3d-unet-99.9/Offline"> 54.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0076 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 144.0
Processors per node: 2
Nodes: 1
Notes: NVIDIA GB200 NVL72, 4-GPU node system
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/coreweave/systems/GB200-NVL_GB200-NVL-186GB_aarch64x4_TRT.json">  (4 x GB200-186GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> coreweave </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.0-0077 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/coreweave/systems/H200-SXM-141GBx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> coreweave </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33066.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34403.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33066.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34403.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>
        </table></div>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<hr>

        <h2 id="results_heading_preview" class="results_table_heading">Datacenter Category: Preview submissions in Closed division</h2>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<div id="results_table_preview" class="resultstable_wrapper"> <table class="resultstable tablesorter tableclosed tabledatacenter" id="results_preview"><thead> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></thead><tfoot> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></tfoot>
        <tr>
        <td class="col-id headcol"> 5.0-0078 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56.0
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/submissions_inference_5.0/tree/main/closed/Google/systems/B200-SXM-180GBx8_TRT.json"> NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 92338.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 97124.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 92203.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 97089.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.0/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 30.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>
        </table></div>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<hr>

<h2 id="count_heading">Count of Results </h2>

    <div class="counttable_wrapper">
    <table class="tablesorter counttable" id="results_summary">
    <thead>
    <tr>
    <th class="count-submitter">Submitter</th>

            <th id="col-llama2-99">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99">GPTJ-99</th>
            <th id="col-gptj-99.9">GPTJ-99.9</th>
            <th id="col-bert-99">Bert-99</th>
            <th id="col-bert-99.9">Bert-99.9</th>
            <th id="col-dlrm-v2-99">Stable Diffusion</th>
            <th id="col-dlrm-v2-99">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9">DLRM-v2-99.9</th>
            <th id="col-retinanet">Retinanet</th>
            <th id="col-resnet50">ResNet50</th>
            <th id="col-3d-unet-99">3d-unet-99</th>
            <th id="col-3d-unet-99.9">3d-unet-99.9</th>
            <th id="all-models">Total</th>
            </tr>
            </thead>
            <tr><td class="count-submitter"> AMD </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td></tr><tr><td class="count-submitter"> ASUSTeK </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 32 </td></tr><tr><td class="count-submitter"> Broadcom_Supermicro </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 8 </td></tr><tr><td class="count-submitter"> Cisco </td><td class="col-result"> 8 </td><td class="col-result"> 4 </td><td class="col-result"> 10 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td><td class="col-result"> 10 </td><td class="col-result"> 4 </td><td class="col-result"> 1 </td><td class="col-result"> 63 </td></tr><tr><td class="count-submitter"> Dell </td><td class="col-result"> 10 </td><td class="col-result"> 10 </td><td class="col-result"> 10 </td><td class="col-result"> 12 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 8 </td><td class="col-result"> 6 </td><td class="col-result"> 5 </td><td class="col-result"> 6 </td><td class="col-result"> 75 </td></tr><tr><td class="count-submitter"> Fujitsu </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 12 </td></tr><tr><td class="count-submitter"> GATEOverflow </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 12 </td></tr><tr><td class="count-submitter"> GigaComputing </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 24 </td></tr><tr><td class="count-submitter"> Google </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 22 </td></tr><tr><td class="count-submitter"> HPE </td><td class="col-result"> 14 </td><td class="col-result"> 14 </td><td class="col-result"> 16 </td><td class="col-result"> 14 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 14 </td><td class="col-result"> 12 </td><td class="col-result"> 14 </td><td class="col-result"> 14 </td><td class="col-result"> 12 </td><td class="col-result"> 7 </td><td class="col-result"> 7 </td><td class="col-result"> 138 </td></tr><tr><td class="count-submitter"> Intel </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 14 </td></tr><tr><td class="count-submitter"> Lambda </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 12 </td></tr><tr><td class="count-submitter"> Lenovo </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 8 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td><td class="col-result"> 8 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 60 </td></tr><tr><td class="count-submitter"> MangoBoost </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td></tr><tr><td class="count-submitter"> NVIDIA </td><td class="col-result"> 12 </td><td class="col-result"> 12 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 52 </td></tr><tr><td class="count-submitter"> Oracle </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 12 </td></tr><tr><td class="count-submitter"> Quanta_Cloud_Technology </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 8 </td><td class="col-result"> 8 </td><td class="col-result"> 8 </td><td class="col-result"> 2 </td><td class="col-result"> 3 </td><td class="col-result"> 67 </td></tr><tr><td class="count-submitter"> Supermicro </td><td class="col-result"> 10 </td><td class="col-result"> 10 </td><td class="col-result"> 6 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 8 </td><td class="col-result"> 4 </td><td class="col-result"> 3 </td><td class="col-result"> 4 </td><td class="col-result"> 67 </td></tr><tr><td class="count-submitter"> Sustainable_Metal_Cloud </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 14 </td></tr><tr><td class="count-submitter"> coreweave </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td></tr>
    <tr>
    <td class="count-submitter">Total</td>
    <td class="col-result"> 94 </td><td class="col-result"> 88 </td><td class="col-result"> 80 </td><td class="col-result"> 80 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 62 </td><td class="col-result"> 44 </td><td class="col-result"> 42 </td><td class="col-result"> 72 </td><td class="col-result"> 64 </td><td class="col-result"> 34 </td><td class="col-result"> 34 </td><td class="col-result"> 698 </td></tr></table></div>
<hr>

    <div id="submittervssubmissionchartContainer" class="bgtext" style="height:370px; width:80%; margin:auto;"></div>
    <div id="modelvssubmissionchartContainer" class="bgtext" style="height:370px; width:80%; margin:auto;"></div>

    <form id="resultSelectionForm" method="post" action="">
        <h3>Select Category and Division</h3>

        <div class="form-field">
            <label for="category">Category</label>
            <select id="category" name="category" class="col">
                <option value='datacenter' selected>Datacenter</option>
<option value='edge' >Edge</option>

            </select>
        </div>

        <div class="form-field">
            <label for="division">Division</label>
            <select id="division" name="division" class="col">
                <option value='closed' selected>Closed</option>
<option value='open' >Open</option>

            </select>
        </div>

        <div class="form-field">
            <label for="with_power">Power</label>
            <select id="with_power" name="with_power" class="col">
                <option value="true" >Performance and Power</option>
                <option value="false" selected>Performance</option>
            </select>
        </div>

        <div class="form-field">
            <button type="submit" name="submit" value="1" id="results_tablesorter">Submit</button>
        </div>
    </form>


<script type="text/javascript">
var sortcolumnindex = 6, perfsortorder = 1;
$('#submittervssubmissionchartContainer').hide();
$('#modelvssubmissionchartContainer').hide();
</script>

<script type="text/javascript" src="javascripts/init_tablesorter.js"></script>
<script type="text/javascript" src="javascripts/results_tablesorter.js"></script>
<script type="text/javascript" src="javascripts/chart_results.js"></script>

</html>







  
  




  



                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["content.tabs.link", "content.code.copy", "navigation.expand", "navigation.sections", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.follow"], "search": "assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
  
      <script src="assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  <script type"text/javascript">
  var resort = true, // re-apply the current sort
        callback = function() {
          // do something after the updateAll method has completed
        };

      // let the plugin know that we made a update, then the plugin will
      // automatically sort the table based on the header settings
      $("table").trigger("updateAll", [ resort, callback ]);
  </script>


  </body>
</html>